{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset and preprocessing\n",
    "The dataset is available at: https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Reading the dataset, Null values are represented by ?\n",
    "data = pd.read_csv(\"diabetic_data.csv\", na_values=[\"?\", 'None'], low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a visual of missing data\n",
    "missingno.matrix(data, figsize = (30,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the proportion of missing values per column\n",
    "prop_na = data.columns.map(lambda x: {x: round(float(data[x].isnull().sum())/len(data[x]),4)} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non informative columns and columns with very high missing values proportion\n",
    "data = data.drop(['encounter_id', 'patient_nbr', 'weight','payer_code', 'medical_specialty', 'max_glu_serum', 'A1Cresult'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get informations about the columns types\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droping missing values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One way to identify categorical variables is to check the number of unique values per column\n",
    "unique_values = data.columns.map(lambda x: {x : data[x].nunique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of diagnoses in the diag1, diag2, diag3 columns is very large. One hot encoding of theses three columns will result in mere than 2000 features\n",
    "diag_1_n = data['diag_1'].value_counts()\n",
    "diag_2_n = data['diag_2'].value_counts()\n",
    "diag_3_n = data['diag_3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are only low proportion of the categories that have a frequency of more than 100 which might cause a problem when splitting the train/test\n",
    "#lets recode the categories with less than min freq as Other\n",
    "min_freq = 100\n",
    "\n",
    "print(\"Diag 1: {} of the catgories appear less than 100 times in the dataset\".format(round(len(diag_1_n[diag_1_n<min_freq])/len(diag_1_n),2)))\n",
    "print(\"Diag 2: {} of the catgories appear less than 100 times in the dataset\".format(round(len(diag_2_n[diag_2_n<min_freq])/len(diag_2_n),2)))\n",
    "print(\"Diag 3: {} of the catgories appear less than 100 times in the dataset\".format(round(len(diag_3_n[diag_3_n<min_freq])/len(diag_3_n),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's recode the categories with a frequency lower than 100 as other\n",
    "\n",
    "diag1_recode  = data['diag_1'].map(lambda x: 'other' if diag_1_n[x] < min_freq else x)\n",
    "diag2_recode  = data['diag_2'].map(lambda x: 'other' if diag_2_n[x] < min_freq else x)\n",
    "diag3_recode  = data['diag_3'].map(lambda x: 'other' if diag_3_n[x] < min_freq else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the changes in a copy of dataset\n",
    "data_rec1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rec1['diag_1'] = diag1_recode\n",
    "data_rec1['diag_2'] = diag2_recode\n",
    "data_rec1['diag_3'] = diag3_recode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the age as ordered factors\n",
    "age_intervals = data_rec1['age'].value_counts().index\n",
    "\n",
    "age_dict ={}\n",
    "compt = 1\n",
    "for k in sorted(age_intervals):\n",
    "    age_dict[k] = compt\n",
    "    compt+=1\n",
    "\n",
    "age_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_intervals = data_rec1['age'].map(lambda x: age_dict[x] )\n",
    "age_intervals.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 0-10 interval is under represented. let's merge it with the 10-20\n",
    "age_intervals = age_intervals.map(lambda x: 2 if x == 1 else x)\n",
    "age_intervals.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rec1['age'] = age_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the columns that contain only a single category\n",
    "data_f1 = data_rec1.copy()\n",
    "cols = data_f1.columns\n",
    "\n",
    "for col in cols:\n",
    "    if data_f1[col].nunique() < 2:\n",
    "        data_f1.drop([col], axis=1, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data_f1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['race', 'gender', 'age', 'admission_type_id',\n",
    "       'discharge_disposition_id', 'admission_source_id', 'diag_1',\n",
    "       'diag_2', 'diag_3', 'number_diagnoses', 'metformin', 'repaglinide',\n",
    "       'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide',\n",
    "       'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone',\n",
    "       'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide',\n",
    "       'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "       'glimepiride-pioglitazone', 'metformin-pioglitazone', 'change',\n",
    "       'diabetesMed', 'readmitted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how the categorical columns are balanced\n",
    "def check_features_balance(data, cat_cols):\n",
    "    for col in cat_cols:\n",
    "        print(col)\n",
    "        print( data[col].value_counts())\n",
    "        print('-'*40)\n",
    "\n",
    "check_features_balance(data_f1,  data_f1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is clear that few of the columns are not informative  as they contain 2 categories of which one is very underrepresented.\n",
    "#Removing: 'acetohexamide', 'tolbutamide', 'miglitol', 'glipizide-metformin', 'glimepiride-pioglitazone'\n",
    "cat_columns = ['race', 'gender', 'age', 'admission_type_id',\n",
    "       'discharge_disposition_id', 'admission_source_id', 'diag_1',\n",
    "       'diag_2', 'diag_3', 'metformin', 'repaglinide',\n",
    "       'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "       'glipizide', 'glyburide', 'pioglitazone',\n",
    "       'rosiglitazone', 'acarbose', 'troglitazone', 'tolazamide',\n",
    "       'insulin', 'glyburide-metformin',\n",
    "       'metformin-pioglitazone', 'change',\n",
    "       'diabetesMed', 'readmitted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f2 = data_f1.copy()\n",
    "data_f2 = data_f2[cat_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check again how the feature distributions\n",
    "check_features_balance(data_f2, cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There still features with  categories that are highly underrepresented\n",
    "#let's recode the categories that have less than min_freq as 'other'. \n",
    "#For measurement columns having 'Up' and 'Down' as values, merge these in a single category called abnormal\n",
    "\n",
    "min_freq = 100\n",
    "\n",
    "for col in cat_columns:\n",
    "    if set(['Up', 'Down']).issubset(list(data_f2[col])):\n",
    "        #Replace all values \"Up\" and \"Down\" with \"Abnormal\"\n",
    "        new_vec = data_f2[col].map(lambda x : 'Abnormal' if x in[\"Up\", 'Down'] else x)\n",
    "        data_f2[col] = new_vec\n",
    "    else:\n",
    "        #The categories with a frequency of less than 100 are recoded as 'other'\n",
    "        count = data_f2[col].value_counts()\n",
    "        new_vec = data_f2[col].map(lambda x : 'other' if count[x] < min_freq else x)\n",
    "        data_f2[col] = new_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check again how the feature distributions\n",
    "check_features_balance(data_f2, cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are still few features with low frequencies for few of their categories. let's drop them\n",
    "cols_to_remove = ['nateglinide', 'chlorpropamide', 'acarbose', 'troglitazone', 'tolazamide', 'glyburide-metformin',\n",
    "                 'metformin-pioglitazone']\n",
    "data_f2.drop(cols_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Let's reform our dataset by concatenating categorical features and continuous one\n",
    "data_f2 = pd.concat([data_f2, data.loc[:,['time_in_hospital',\n",
    "       'num_lab_procedures', 'num_procedures', 'num_medications',\n",
    "       'number_outpatient', 'number_emergency', 'number_inpatient']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the single row having 'other' as value\n",
    "data_f2 = data_f2[data_f2['gender']!='other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis and dataset preparation for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the categories of the outcome are balanced \n",
    "data_f2['readmitted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the features from the outcome\n",
    "X = data_f2.drop(['readmitted'], axis=1)\n",
    "Y = data_f2['readmitted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this analysis, let's consider only 2 groups: Admitted:0, non admitted:1\n",
    "Y_new = Y.map(lambda x : 0 if x == \"NO\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_new.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's split separate the categorical columns from the continuous columns for data exploration\n",
    "continuous_columns = ['time_in_hospital',\n",
    "       'num_lab_procedures', 'num_procedures', 'num_medications',\n",
    "       'number_outpatient', 'number_emergency', 'number_inpatient']\n",
    "\n",
    "categorical_columns = [i for i in X.columns if i not in continuous_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = X.loc[:, categorical_columns]\n",
    "X_cont = X.loc[:, continuous_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize categorical data\n",
    "X_cat_vis = X_cat\n",
    "X_cat_vis['readmitted'] = Y_new\n",
    "X_cat_vis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot categorical variable\n",
    "fig, axs = plt.subplots(ncols=1)\n",
    "for i, col in enumerate(X_cat_vis.columns):\n",
    "    if col not in ['readmitted', 'diag_1', 'diag_2', 'diag_3']:\n",
    "        plt.figure(i)       \n",
    "        sns.countplot(x='readmitted', data=X_cat_vis, hue=col)\n",
    "        plt.title(col.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize continuous data\n",
    "X_cont_vis = X_cont\n",
    "X_cont_vis['readmitted'] = Y_new\n",
    "X_cont_vis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot continuous variable\n",
    "fig, axs = plt.subplots(ncols=1)\n",
    "\n",
    "for i, col in enumerate(X_cont_vis.columns):\n",
    "    if col !='readmitted':\n",
    "        plt.figure(i)    \n",
    "        sns.boxplot(x='readmitted', y=col, data=X_cont_vis)\n",
    "        plt.title(col.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get statistics from continuous values\n",
    "X[continuous_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(X_cont_vis, hue='readmitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get heatmap of correlations\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(X.corr(), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot encode categorical data\n",
    "X_final = pd.get_dummies(X, prefix=None, prefix_sep='_', \n",
    "                        dummy_na=False, columns=categorical_columns, \n",
    "                        sparse=False, \n",
    "                        drop_first=True, dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final['readmitted'] = Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy of the preprocessed data\n",
    "X_final.to_csv(\"1_dataset_ML_ready.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remain to do\n",
    "Dimensionality reduction\n",
    "Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
