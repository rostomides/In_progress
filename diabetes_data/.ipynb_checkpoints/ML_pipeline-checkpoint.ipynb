{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before loading the following dataset, be sure to run the preprocessing ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"1_dataset_ML_ready.csv\", index_col=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Custom helper function\n",
    "\n",
    "I have created a custom gridSearch function that is 'Fault tolerant' and which prevent the gridSearch from craching in case of incompatible arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create all tuples of parameters\n",
    "from itertools import product #Returns the cartesian product of lists (same as nested for loops)\n",
    "def All_params_grid(dico):\n",
    "    #Create list of keys and a list of values\n",
    "    keys = list(dico.keys())\n",
    "    values = dico.values()\n",
    "    #Create all possible combinations of parameters\n",
    "    params = []    \n",
    "    for value in product(*values):#Loop trough all the combination of values\n",
    "        subParam =dict() #reconstruct the dictionary of parameters\n",
    "        for i in range(len(keys)):           \n",
    "            subParam[str(keys[i])] = value[i] #Attribute the respective value to the respective key\n",
    "        params.append(subParam)\n",
    "    return(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "#Custom training method with cross validation\n",
    "def trainRawModel(strModel,params, x_train, y_train, x_test, y_test):\n",
    "    '''This function is used to train the model without any ensemble method'''    \n",
    "    #Instanciate the model\n",
    "    model = eval(strModel)()\n",
    "    #Set parameters\n",
    "    model.set_params(**params)\n",
    "    \n",
    "    #Train Base model No ensemble           \n",
    "    model_raw = model.fit(x_train,y_train)        \n",
    "    cv = cross_validate(model_raw, x_train, y_train, cv=10, scoring=['recall', 'roc_auc', 'f1'], \n",
    "                        return_train_score=False,n_jobs=5)        \n",
    "    model_raw.fit(x_train,y_train)\n",
    "\n",
    "    #Predict model        \n",
    "    y_pred_raw = model_raw.predict(x_test)\n",
    "\n",
    "    roc_predict_raw = roc_auc_score(y_test, y_pred_raw, average='macro', sample_weight=None)\n",
    "    obj = {\n",
    "        'model_raw': model_raw,\n",
    "        'cv_raw': cv,\n",
    "        'roc_predict_raw': roc_predict_raw\n",
    "    }        \n",
    "    return(obj) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Gridsearch function \n",
    "def custom_gridSearch(strModel, param_dict, x_train, y_train, x_test, y_test):\n",
    "    params = All_params_grid(param_dict)\n",
    "    best_model = ''\n",
    "    \n",
    "    for param in params:  \n",
    "        try:\n",
    "            model = trainRawModel(strModel,param, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "        \n",
    "            if best_model == '':\n",
    "                best_model = model\n",
    "            elif best_model['roc_predict_raw'] < model['roc_predict_raw']:\n",
    "                best_model = model\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return best_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply adaptive boosting classifier on a already tuned model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def ada_boost(model, x_train, y_train, x_test, y_test):\n",
    "        #Try model with boosting\n",
    "        try:\n",
    "            adaGS_Param ={\n",
    "                'algorithm':['SAMME'],\n",
    "                'base_estimator':[model['model_raw']],\n",
    "                'n_estimators':[50, 80,100,500]            \n",
    "            }\n",
    "            adaBoost = AdaBoostClassifier()\n",
    "\n",
    "            adaBGS = GridSearchCV(adaBoost, adaGS_Param, cv=10, error_score=-1, scoring='roc_auc')\n",
    "            #Training a model\n",
    "            adaBGS.fit(x_train,y_train)        \n",
    "        \n",
    "            adaBest = adaBGS.best_estimator_\n",
    "            \n",
    "            y_pred_adaB = adaBGS.best_estimator_.predict(X_test)\n",
    "            \n",
    "            adaScore =  roc_auc_score(y_test, y_pred_adaB, average='macro', sample_weight=None)\n",
    "            \n",
    "        #Return final object\n",
    "        except:\n",
    "            adaBest=None\n",
    "            adaScore=0\n",
    "            \n",
    "        #bagging\n",
    "                               \n",
    "        obj = {\n",
    "            'initial_model' : model,\n",
    "            'model_adaBoost': adaBest,\n",
    "            'adaBoost_score':adaScore            \n",
    "        }\n",
    "        \n",
    "        return(obj)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply a bagging classifier on a already tuned model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "def baggingModel(model, x_train, y_train, x_test, y_test):   \n",
    "    \n",
    "        params = {   \n",
    "            'base_estimator':[model['model_raw']],\n",
    "            'max_features' : [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "            'max_samples' : [0.05, 0.1, 0.2, 0.5]\n",
    "        }\n",
    "        try:\n",
    "            \n",
    "            BaggingModel = BaggingClassifier()\n",
    "            GS = GridSearchCV(BaggingModel, params, cv=5, scoring='roc_auc')\n",
    "            GS.fit(x_train, y_train)\n",
    "            \n",
    "            bestBag = GS.best_estimator_\n",
    "\n",
    "            y_pred_bagging = bestBag.predict(X_test)\n",
    "\n",
    "            model_bagging_score = roc_auc_score(y_test,y_pred_bagging, average='macro', sample_weight=None)\n",
    "            \n",
    "        except:\n",
    "            bestBag = None,\n",
    "            model_bagging_score = 0\n",
    "        \n",
    "        \n",
    "        obj ={\n",
    "            'initial_model' : model,\n",
    "            'model_bagging' : bestBag,\n",
    "            'bagging_score': model_bagging_score\n",
    "            }\n",
    "        \n",
    "        return(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform the predictions\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "#Available Classifiers\n",
    "# classifiers =['DecisionTreeClassifier', \"LogisticRegression\", \"XGBClassifier\", \n",
    "#               'SVC', 'RandomForestClassifier', \"GradientBoostingClassifier\", \n",
    "#               \"PassiveAggressiveClassifier\", \"SGDClassifier\" ]\n",
    "\n",
    "def prediction_pipeline(classifiers_list, X_train_scaled, y_train, X_test_scaled, y_test):\n",
    "    #Train all models    \n",
    "    All_classifiers= defaultdict(dict)\n",
    "    compt = 1\n",
    "    for classifier in classifiers_list:\n",
    "        print('Current classifier ' + classifier)\n",
    "        \n",
    "        start = datetime.now()\n",
    "        \n",
    "        model = custom_gridSearch(classifier, model_parameters[classifier], X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "        All_classifiers[classifier][classifier + '_raw'] = model\n",
    "\n",
    "        #Store intermediate versions\n",
    "        pickling_on = open(\"zz_model_{}_out_of_{}.pickle\".format(compt, len(classifiers)),\"wb\")\n",
    "        pickle.dump(All_classifiers, pickling_on)\n",
    "        pickling_on.close()\n",
    "        \n",
    "        \n",
    "        end = datetime.now()\n",
    "        diff = (start - end)\n",
    "        hours = diff//3600\n",
    "        minutes = (diff - (hours * 3600))//60\n",
    "        seconds = diff - (hours * 3600)-(minutes * 60)\n",
    "\n",
    "        print(classifier + \" ended after \" + str(hours) + \" hours, \" + str(minutes) + \" minutes and  \" + str(seconds)+ \" seconds.\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #take a sample for testing the code\n",
    "# import random\n",
    "# indecies = random.sample(range(len(data)), 500)\n",
    "# data = data.iloc[indecies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('readmitted', axis=1)\n",
    "Y = data['readmitted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into training testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, Y, test_size=0.1, random_state=101)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Scaler = StandardScaler()\n",
    "X_train_scaled = Scaler.fit_transform(X_train)\n",
    "X_test_scaled = Scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a dictionary of model parameters\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from xgboost import XGBClassifier\n",
    "n_jobs = 5\n",
    "model_parameters = {\n",
    "    'DecisionTreeClassifier':{    \n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth':[None], #integer\n",
    "        'min_samples_split': [2, 4, 8, 20], #integer or proportion of samples\n",
    "        'min_samples_leaf':[1, 5, 10 , 20],\n",
    "        'max_features':['auto', 'log2', None],\n",
    "        'max_leaf_nodes' :[None], #int\n",
    "        'min_impurity_decrease' :[1e-7],    \n",
    "        'class_weight':['balanced'],    \n",
    "    },\n",
    "    #-------------\n",
    "    \"LogisticRegression\":{\n",
    "        'penalty':['l2','l1'], # The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties\n",
    "        'dual':[False], #Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.\n",
    "        'tol':[0.0001], #Tolerance for stopping criteria\n",
    "        'C':[1.0], #Like in support vector machines, smaller values specify stronger regularization.\n",
    "        'fit_intercept':[True], \n",
    "        'intercept_scaling':[1], \n",
    "        'class_weight':['balanced'], \n",
    "        'random_state':[101], \n",
    "        'solver':[\n",
    "#             'liblinear', #Good choice for small datasets, one-vs-rest only, handels L1\n",
    "                  'newton-cg',  #multinomial problems (multi-class), L2\n",
    "                  'lbfgs',  #multinomial problems (multi-class), L2\n",
    "#                   'sag', #Stochastic Average Gradient descent #Good choice for small datasets, multinomial problems, L2\n",
    "#                   'saga', #Good choice for small datasets, multinomial problems, handels L1\n",
    "                 ], \n",
    "        'max_iter':[100, 500], #Useful only for the newton-cg, sag and lbfgs solvers\n",
    "        'multi_class':[\n",
    "#             'ovr', \n",
    "#                        'multinomial', \n",
    "                       'auto'], #‘ovr’, ‘multinomial’, ‘auto’\n",
    "        'verbose':[0], \n",
    "        'warm_start':[False], \n",
    "        'n_jobs':[n_jobs]\n",
    "    },\n",
    "    #-------------\n",
    "    'SVC':{\n",
    "        'C':[0.5, 1.0, 2, 5], \n",
    "        'kernel':['linear', 'poly', 'rbf', 'sigmoid'], \n",
    "        'degree':[3,4,5], #degree of the polynomial (only for polynomial kernel)\n",
    "        'gamma': ['auto', 'scale'],\n",
    "        'coef0': [0.0, 1, 2], \n",
    "        'shrinking':[True], \n",
    "        'probability':[False],  #Change this to True if probabilities need to be calculated\n",
    "        'tol': [0.001], \n",
    "        'cache_size':[200], \n",
    "        'class_weight':['balanced'], \n",
    "        'verbose':[False], \n",
    "        'max_iter':[-1], \n",
    "        'decision_function_shape':['ovr', 'ovo'], \n",
    "        'random_state': [101]    \n",
    "    },\n",
    "    #-------------\n",
    "    'RandomForestClassifier':{\n",
    "        'n_estimators':[100, 200, 1000],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth':[None], #integer\n",
    "        'min_samples_split': [2, 8, 20], #integer or proportion of samples\n",
    "        'min_samples_leaf':[1, 5, 10, 20],\n",
    "        'max_features':['auto', 'log2', None],\n",
    "        'max_leaf_nodes' :[None], #int\n",
    "        'min_impurity_decrease' :[1e-7],    \n",
    "        'class_weight':['balanced', 'balanced_subsample'],    \n",
    "        'bootstrap':[True],\n",
    "        'n_jobs':[n_jobs]\n",
    "    },\n",
    "    #-------------\n",
    "    \"GradientBoostingClassifier\": {\n",
    "        'loss':['deviance'], \n",
    "        'learning_rate':[0.1], \n",
    "        'n_estimators':[100, 200, 1000], \n",
    "        'subsample':[1.0], \n",
    "        'criterion': ['friedman_mse'], \n",
    "        'min_samples_split':[2, 5, 10, 20], \n",
    "        'min_samples_leaf': [2, 5, 10, 20], \n",
    "        'min_weight_fraction_leaf':[0.0], \n",
    "        'max_depth':[3, 4, 5, 10], \n",
    "        'min_impurity_decrease': [0.0], \n",
    "        'min_impurity_split': [None], \n",
    "        'init': [None], \n",
    "        'random_state':[101], \n",
    "        'max_features':[None, 'sqrt', 'log2'], \n",
    "        'verbose':[0], \n",
    "        'max_leaf_nodes':[None], \n",
    "        'warm_start':[False], \n",
    "        'presort':['auto'], \n",
    "        'validation_fraction': [0.1], \n",
    "        'n_iter_no_change':[5], #if the score is not improving for 3 iterations ==>Early stopping. Default None\n",
    "        'tol':[0.0001]\n",
    "    },\n",
    "    #-------------------------------\n",
    "    \"SGDClassifier\":{\n",
    "        'loss':['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'], \n",
    "        'penalty':['l2', 'l1', 'elasticnet'], \n",
    "        'alpha': [0.0001], \n",
    "        'l1_ratio': [0.15, 0.4], \n",
    "        'fit_intercept':[True], \n",
    "        'max_iter':[1000], \n",
    "        'tol': [0.001], \n",
    "        'shuffle':[True], \n",
    "        'verbose':[0], \n",
    "        'epsilon':[0.1], \n",
    "        'n_jobs':[n_jobs], \n",
    "        'random_state':[101], \n",
    "        'learning_rate':['optimal', 'adaptive', 'invscaling', 'constant'], \n",
    "        'eta0':[0.0], \n",
    "        'power_t':[0.5], \n",
    "        'early_stopping':[True], \n",
    "        'validation_fraction':[0.1], \n",
    "        'n_iter_no_change': [10], \n",
    "        'class_weight':['balanced'], \n",
    "        'warm_start':[False], \n",
    "        'average':[False]\n",
    "    },\n",
    "    #-------------------------------    \n",
    "    \"PassiveAggressiveClassifier\": {\n",
    "        'C':[1.0], \n",
    "        'fit_intercept':[True], \n",
    "        'max_iter':[1000], \n",
    "        'tol': [0.001], \n",
    "        'early_stopping':[True], \n",
    "        'validation_fraction':[0.1], \n",
    "        'n_iter_no_change': [10], \n",
    "        'shuffle':[True], \n",
    "        'verbose':[0], \n",
    "        'loss':['hinge', 'squared_hinge'], \n",
    "        'n_jobs':[n_jobs], \n",
    "        'random_state':[101], \n",
    "        'warm_start':[False], \n",
    "        'class_weight':['balanced'], \n",
    "        'average':[False]\n",
    "    },\n",
    "    #-------------------------------    \n",
    "    \"XGBClassifier\": {\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'learning_rate': [0.1],\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'verbosity':[0],\n",
    "        'silent': [None],\n",
    "        \"objective\": ['binary:logistic'],\n",
    "        'booster': ['gbtree'],\n",
    "        'nthread': [None],\n",
    "#         'gamma': [0.5, 1, 1.5, 2, 5],        \n",
    "#         'min_child_weight': [1, 5, 10],        \n",
    "#         'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'colsample_bylevel': [1],\n",
    "        'colsample_bynode': [1],\n",
    "        'reg_alpha': [1],\n",
    "        'reg_lambda': [1],\n",
    "        'scale_pos_weight': [1],\n",
    "        'base_score': [0.5],\n",
    "        'random_state': [101],\n",
    "        'seed': [None],\n",
    "        'missing': [None],        \n",
    "        'n_jobs':[-1]                   \n",
    "    }    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-learn classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available Classifiers\n",
    "#classifiers =['DecisionTreeClassifier', \"LogisticRegression\", \"XGBClassifier\", 'SVC', 'RandomForestClassifier', \"GradientBoostingClassifier\", \"PassiveAggressiveClassifier\", \"SGDClassifier\", ]\n",
    "\n",
    "classifiers =['DecisionTreeClassifier', \"LogisticRegression\", \n",
    "#               \"XGBClassifier\", \n",
    "              'SVC', 'RandomForestClassifier', \"GradientBoostingClassifier\", \n",
    "              \"PassiveAggressiveClassifier\", \"SGDClassifier\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"once\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the prediction pipeline\n",
    "prediction_pipeline(classifiers, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# #classifiers =['DecisionTreeClassifier', \"LogisticRegression\", \"XGBClassifier\", 'SVC', 'RandomForestClassifier', \"GradientBoostingClassifier\", \"PassiveAggressiveClassifier\", \"SGDClassifier\", ]\n",
    "\n",
    "\n",
    "# classifiers = [('lr', lr['model_raw']), ('svc', svc['model_raw']), \n",
    "#                ('rf', rf['model_raw']), ('gb', gb['model_raw']), \n",
    "#                ('sgd', sgd['model_raw']), ('passive_aggressive', p_agg['model_raw']),\n",
    "#               ('GaussianNB', nb['model_raw'])]\n",
    "\n",
    "# Voting_classifier = VotingClassifier(estimators=classifiers, voting='hard', n_jobs=-1)\n",
    "\n",
    "# Voting_classifier.fit(X_train_scaled, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
